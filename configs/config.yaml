training:  
  run: PPO
  # Run name
  name: PPO-1
  checkpoint_freq: 200
  resume: True
  # Directory to write logs in
  local_dir: "./results/"
  stop:
      # Time limit per trial (in seconds)
      # time_total_s: 20000 
      # timesteps_total: 1.e+10
      training_iteration: 20000
  
  ## ======== COMMON CONFIG ======== 
  # See https://docs.ray.io/en/master/rllib-training.html#common-parameters
  common_config:
    framework: torch
    eager_tracing: False
    log_level: WARNING
    # Training will not work with poke-env if this value != 0
    num_workers: 0
    num_envs_per_worker: 1
    num_cpus_per_worker: 1
    # num_gpus: 1
    gamma: 0.99

    # === ENVIRONMENT CONFIG ===
    #  /!\ The env should be registered in train script
    env: pokeEnv
    env_config:
      battle_format: gen8randombattle

    # === MODEL CONFIG ===
    model:
      custom_model: poke_model
      custom_model_config: 
        dense_size: 64
        lstm_state_size: 256

  ## ======== AGENT SPECIFIC CONFIG ======== 
  # You can override common config if needed
  # See https://docs.ray.io/en/master/rllib-algorithms.html

  specific_config:
    PPO:
      ## ======== PPO SPECIFIC CONFIG ======== 
      # Should use a critic as a baseline (otherwise dont use value baseline;
      # required for using GAE).
      use_critic: True
      # If true use the Generalized Advantage Estimator (GAE)
      # with a value function see https://arxiv.org/pdf/1506.02438.pdf.
      use_gae: True
      # The GAE (lambda) parameter.
      lambda: 0.95
      # Initial coefficient for KL divergence.
      kl_coeff: 0.2
      # Size of batches collected from each worker.
      rollout_fragment_length: 256
      # Number of timesteps collected for each SGD round.
      train_batch_size: 256
      # Total SGD batch size across all devices for SGD. This defines the
      # minibatch size within each epoch.
      sgd_minibatch_size: 256
      # Whether to shuffle sequences in the batch when training (recommended).
      shuffle_sequences: True
      # Number of SGD iterations in each outer loop (i.e. number of epochs to
      # execute per train batch).
      num_sgd_iter: 10
      lr: 2.e-4
      # Learning rate schedule.
      lr_schedule: null
      # Coefficient of the value function loss. IMPORTANT: you must tune this if
      # you set vf_share_layers=True inside your models config.
      vf_loss_coeff: 0.5
      # Coefficient of the entropy regularizer.
      entropy_coeff: 0.001
      # Decay schedule for the entropy regularizer.
      entropy_coeff_schedule: null
      # PPO clip parameter.
      clip_param: 0.3
      # Clip param for the value function. Note that this is sensitive to the
      # scale of the rewards. If your expected V is large increase this.
      vf_clip_param: 10.0
      # If specified clip the global norm of gradients by this amount.
      grad_clip: null
      # Target value for KL divergence.
      kl_target: 0.01
      # Whether to rollout complete_episodes or truncate_episodes.
      batch_mode: complete_episodes
      # Which observation filter to apply to the observation.
      observation_filter: NoFilter

    IMPALA:
      ## ======== IMPALA SPECIFIC CONFIG ======== 
      # V-trace params (see vtrace_tf/torch.py).
      vtrace: True
      vtrace_clip_rho_threshold: 1.0
      vtrace_clip_pg_rho_threshold: 1.0
      # System params.
      #
      # == Overview of data flow in IMPALA ==
      # 1. Policy evaluation in parallel across `num_workers` actors produces
      #    batches of size `rollout_fragment_length * num_envs_per_worker`.
      # 2. If enabled, the replay buffer stores and produces batches of size
      #    `rollout_fragment_length * num_envs_per_worker`.
      # 3. If enabled, the minibatch ring buffer stores and replays batches of
      #    size `train_batch_size` up to `num_sgd_iter` times per batch.
      # 4. The learner thread executes data parallel SGD across `num_gpus` GPUs
      #    on batches of size `train_batch_size`.
      #
      rollout_fragment_length: 50
      train_batch_size: 500

      min_iter_time_s: 10
      # set >1 to load data into GPUs in parallel. Increases GPU memory usage
      # proportionally with the number of buffers.
      num_data_loader_buffers: 1
      # how many train batches should be retained for minibatching. This conf
      # only has an effect if `num_sgd_iter > 1`.
      minibatch_buffer_size: 1
      # number of passes to make over each train batch
      num_sgd_iter: 1
      # set >0 to enable experience replay. Saved samples will be replayed with
      # a p:1 proportion to new data samples.
      replay_proportion: 2.0
      # number of sample batches to store for replay. The number of transitions
      # saved total will be (replay_buffer_num_slots * rollout_fragment_length).
      replay_buffer_num_slots: 300
      # max queue size for train batches feeding into the learner
      learner_queue_size: 16
      # wait for train batches to be available in minibatch buffer queue
      # this many seconds. This may need to be increased e.g. when training
      # with a slow environment
      learner_queue_timeout: 500
      # level of queuing for sampling.
      max_sample_requests_in_flight_per_worker: 2
      # max number of workers to broadcast one set of weights to
      broadcast_interval: 1
      # Use n (`num_aggregation_workers`) extra Actors for multi-level
      # aggregation of the data produced by the m RolloutWorkers
      # (`num_workers`). Note that n should be much smaller than m.
      # This can make sense if ingesting >2GB/s of samples, or if
      # the data requires decompression.
      num_aggregation_workers: 0

      # Learning params.
      grad_clip: 20.0
      # either adam or rmsprop
      opt_type: adam 
      lr: 5.e-4
      # lr_schedule: [
      #       [0, 0.0005],
      #       [20000000, 0.000000000001],
      #   ]
      # rmsprop considered
      decay: 0.99
      momentum: 0.0
      epsilon: 0.1

      # balancing the three losses
      vf_loss_coeff: 0.5
      entropy_coeff: 0.01
      entropy_coeff_schedule: null

      # Callback for APPO to use to update KL, target network periodically.
      # The input to the callback is the learner fetches dict.
      after_train_step: null     
